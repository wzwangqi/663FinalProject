\documentclass[11pt]{article}

\usepackage{setspace}
\usepackage{graphicx}
\usepackage{url}
\usepackage{color}
\usepackage{bm}
\usepackage{mathrsfs}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{amssymb}
\usepackage{epstopdf}
\usepackage{array}
\usepackage{subfigure}
\marginparwidth 0pt
\oddsidemargin  0pt
\evensidemargin  0pt
\marginparsep 0pt

\topmargin 0pt
\textwidth   6.5 in
\textheight  8.5 in


\begin{document}

\title{ Spectral Clustering Implementation and Appliacation }
\author{Qi Wang ~~~ Hanqiu Xia}
\maketitle

\abstract{Spectral clustering is widely used in image segmentation. The main idea of spectral clustering is to use the spectrum (eigenvalues) of the similarity matrix of the data to perform dimensionality reduction and then perform clustering in fewer dimensions.  In this project, we investigated the normalized spectral decomposition algorithm from the paper "On Spectral Clustering: Analysis and an algorithm" by A. Ng, M. Jordan, and Y. Weiss [1]. Their algorithm improved upon the existing spectral clustering algorithm by resolving the issues of inconsistent algorithms of using eigenvectors as well as providing evidence of it resulting in a reasonable clustering. We implemented the algorithm using Python with considering both common and edge cases. We also optimized the Python codes by applying vectorization, Cython, and Just-In-Time compiling. Finally, we tested the algorithm on simulated and real datasets and compare the experimental results with those of k-means clustering to see if spectral clustering dramatically improves the results.}

\section{Background}

The research paper we have chosen is "On Spectral Clustering: Analysis and an algorithm" by A. Ng, M. Jordan, and Y. Weiss [1]. Spectral clustering is inspired by the the idea of spectral graph partitioning, in which we use the first two eigenvectors to partition the graph into exactly two parts. It is widely used in image segmentation. The other clustering algorithms such as K-means can have many local minima and require mutiple restarts to find a good solution using iterative algorithms. Spectral clustering serves as a promising alternative.According to Ng et al [1].,  the basic idea of spectral clustering is utilising the top  $k$ eigenvectors simultaneously to cluster points into  $k$ subsets. Despite its sucess, many authors disagree on exactly how to use the eigenvectors. The detailed algorithm proposed by Ng, Jordan, and Weiss is as follows:\\[10pt]
\qquad Suppose we have a set of $n$ points $S = \{ s_1, \ldots, s_n\}$ in ${\rm I\!R}^{m}$, and we want to cluster them into $k$ groups.\\

\textbf{Step 1:} Construct the affinity matrix $A \in{\rm I\!R}^{n\times n} $,  each element in A is defined as $A_{ij} = exp(-||s_i-s_j||^2/2\sigma^2)$,  for $i, j = 1,\ldots, n$. We will introduce a method of choosing $\sigma$ in later pages.\\

\textbf{Step 2:} Define $D$ to be the diagonal matrix with $D_{ii} =\sum_{j=1}^{n} A _{ij}$, and form the normalized Laplacian matrix $L = D^{-1/2}AD^{-1/2}$.\\

\textbf{Step 3:} Capture the fist $k$ largest eigenvectors of $L, e_1, e_2,  \ldots, e_k, e_i \in {\rm I\!R}^{n} $ and form the matrix $E = [e_1 ~e_2 ~\cdots ~e_k ] \in  {\rm I\!R}^{n\times k}.$\\

\textbf{Step 4:} Create the new normalized matrix $U \in {\rm I\!R}^{n\times k}$ from $E$, defined as $U_{ij}= E_{ij}/(\sum_{j=1}^{n}E_{ij}^2)^{1/2}$. \\

\textbf{Step 5:} Consider $U$ to be a set of $n$   points that need to be clustered now, apply K-means or any other algorithm that can minimize distortion to cluster $U$. \\

\textbf{Step 6:}  Assign the original point $s_i$ in to cluster $j$ if and only if the $i$th-row of $U$ was distributed to cluster $j$  in previous step.       \\

It improved upon the spectral clustering algorithm by suggesting the normalized spectral decomposition in forming the affinity matrix. Their algorithm resolved the problem of inconsistent algorithms of using eigenvectors in spectral clustering. When applying the algithm to data that does not have a clear segmentation, it performs much better than the other clustering algorithms. We will implement the spectral clustering algorithm and optimize the coding, and then we will utilize it on real datasets and compare the results with other clustering mechanisms such as k-means clustering.



\section{Implementation}
We have written four functions to implement the algorithm. The first function GenerateData generates data for the ideal case. The second function Circle generates data for the general case. The third function CalculateAffinity calculates the affinity matrix which is the first step in the above described algorithm. The last function Spectral implements Step 2 to 4 in the algorithm and creates a normalized matrix and performs k-means clustering to create cluster labels (Step 5 and 6).

\subsection{Ideal Case}
The ideal case denotes the situation when the clusters are clearly partitioned and are far apart from each other. Our first function GenerateData generates data that falls under this category. The proposed mechanism works very well in this case and will result in an exact match to the true clustering of the original data.
\subsection{General Case}
K-means clustering do not perform well with convex regions or not clearly separated clusters. Using spectral clustering here performs a much better result as it is better suited for tights clusters. Spectral clustering will form tight clusters around well-separated points on the surface of the sphere according to their "true" cluster. Looking at the generated plots using both algorithms, we can see that spectral clustering performs much better in covex cases.

\section{Testing}
We used pytest-ipynb to conduct unit testing. The testing file is named test*.ipynb, and by running through the file in terminal by the command py.test, the unit testing is run. All tests are passed. For each function, there are three tests to assert that each function creates data in the correct format and correct dimension to test for general cases. And by setting the seed, we can confirm that each function creates the correct matrix. For each function, there are two tests for edge cases. For the edge cases, all functions should generate an error for the incorrect inputs such as non-arrays. 

\section{Optimization}
We used vectorization, cython, and Just-In-Time compiling to optimize our code.
\subsection{Vectorization}
To eliminate the use of for loops, we vectorized to code to improve the performance. If we have a large dataset to perform clustering, the runtime will improve consistently by at least a factor of two.
\subsection{JIT (Just-In-Time compiling) }
We also used JIT to improve the performance, and as a result, JIT improves the runtime the most. It improved by around 20 fold.
\subsection{Cython}
Cython does not perform quite as well compared to the above two methods. It only improves the runtime by around a factor of 1.5.

\subsection{Improvement result}
We can see that JIT produces the best results in optimization. Thus, we will use JIT in the following applications.

\section{Application and Comparison}
\subsection{Application on Simulated Data}
The first set of simulated data we used fall under the ideal case. The data points are well segmented and from the graph we can see that they form three clusters perfectly. Another set of simulated data tests for the general case. This set of data forms three circles.
\subsection{Application on Real Data}
We will now apply the algorithm to the iris dataset.
\subsection{Comparison of Sepctral clustering and K-means}



\section{Conclusion}

%\bibliographystyle{plain}
%\bibliography{bibliography}


\newpage
\begin{thebibliography}{9}



\bibitem{nips} 
A. Ng, M. Jordan, and Y. Weiss.
On Spectral Clustering: Analysis and an algorithm.
\textit{Advances in Neural Information Processing}.
Vol. 14, No. 2. (2001), pp. 849-856 

\end{thebibliography}



\end{document}