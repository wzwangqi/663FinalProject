\documentclass[11pt]{article}

\usepackage{setspace}
\usepackage{graphicx}
\usepackage{url}
\usepackage{color}
\usepackage{bm}
\usepackage{mathrsfs}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{amssymb}
\usepackage{epstopdf}
\usepackage{array}
\usepackage{subfigure}
\marginparwidth 0pt
\oddsidemargin  0pt
\evensidemargin  0pt
\marginparsep 0pt

\topmargin 0pt
\textwidth   6.5 in
\textheight  8.5 in


\begin{document}

\title{ Spectral Clustering Implementation and Appliacation }
\author{Qi Wang ~~~ Hanqiu Xia}
\maketitle

\abstract{Spectral clustering is widely used in image segmentation. The main idea of spectral clustering is to use the spectrum (eigenvalues) of the similarity matrix of the data to perform dimensionality reduction and then perform clustering in fewer dimensions.  In this project, we investigated the normalized spectral decomposition algorithm from the paper "On Spectral Clustering: Analysis and an algorithm" by A. Ng, M. Jordan, and Y. Weiss [1]. Their algorithm improved upon the existing spectral clustering algorithm by resolving the issues of inconsistent algorithms of using eigenvectors as well as providing evidence of it resulting in a reasonable clustering. We implemented the algorithm using Python with considering both common and edge cases. We also optimized the Python codes by applying vectorization, Cython, and Just-In-Time compiling. Finally, we tested the algorithm on simulated and real datasets and compare the experimental results with those of k-means clustering to see if spectral clustering dramatically improves the results.}

\section{Background}

Spectral clustering is inspired by the the idea of spectral graph partitioning, in which we use the first two eigenvectors to partition the graph into exactly two parts. According to Ng et al [1].,  the basic idea of spectral clustering is utilising the  $k$ eigenvectors simultaneously to cluster points into  $k$ subsets. The detailed algorithm is as follows:\\[10pt]
\qquad Suppose we have a set of $n$ points $S = \{ s_1, \ldots, s_n\}$ in ${\rm I\!R}^{m}$, and we want to cluster them into $k$ groups.\\

\textbf{Step 1:} Construct the affinity matrix $A \in{\rm I\!R}^{n\times n} $,  each element in A is defined as $A_{ij} = exp(-||s_i-s_j||^2/2\sigma^2)$,  for $i, j = 1,\ldots, n$. We will introduce a method of choosing $\sigma$ in later pages.\\

\textbf{Step 2:} Define $D$ to be the diagonal matrix with $D_{ii} =\sum_{j=1}^{n} A _{ij}$, and form the normalized Laplacian matrix $L = D^{-1/2}AD^{-1/2}$.\\

\textbf{Step 3:} Capture the fist $k$ largest eigenvectors of $L, e_1, e_2,  \ldots, e_k, e_i \in {\rm I\!R}^{n} $ and form the matrix $E = [e_1 ~e_2 ~\cdots ~e_k ] \in  {\rm I\!R}^{n\times k}.$\\

\textbf{Step 4:} Create the new normalized matrix $U \in {\rm I\!R}^{n\times k}$ from $E$, defined as $U_{ij}= E_{ij}/(\sum_{j=1}^{n}E_{ij}^2)^{1/2}$. \\

\textbf{Step 5:} Consider $U$ to be a set of $n$   points that need to be clustered now, apply K-means or any other algorithm that can minimize distortion to cluster $U$. \\

\textbf{Step 6:}  Assign the original point $s_i$ in to cluster $j$ if and only if the $i$th-row of $U$ was distributed to cluster $j$  in previous step.       \\


\section{Implementation}
\subsection{Ideal Case }
\subsection{General Case}


\section{Testing}
\subsection{Unit  Test for Common Case}
\subsection{Unit Test for Edge Case}

\section{Optimization}

\subsection{Vectorization}
\subsection{Cython}
\subsection{JIT (Just-In-Time compiling) }

\subsection{Improvement result}


\section{Application and Comparison}
\subsection{Application on Simulated Data}

\subsection{Application on Real Data}
\subsection{Comparison of Sepctral clustering and K-means}



\section{Conclusion}

%\bibliographystyle{plain}
%\bibliography{bibliography}


\newpage
\begin{thebibliography}{9}



\bibitem{nips} 
A. Ng, M. Jordan, and Y. Weiss.
On Spectral Clustering: Analysis and an algorithm.
\textit{Advances in Neural Information Processing}.
Vol. 14, No. 2. (2001), pp. 849-856 

\end{thebibliography}



\end{document}