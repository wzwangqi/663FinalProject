\documentclass[11pt]{article}

\usepackage{setspace}
\usepackage{graphicx}
\usepackage{url}
\usepackage{color}
\usepackage{bm}
\usepackage{mathrsfs}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{amssymb}
\usepackage{epstopdf}
\usepackage{array}
\usepackage{subfigure}
\marginparwidth 0pt
\oddsidemargin  0pt
\evensidemargin  0pt
\marginparsep 0pt

\topmargin 0pt
\textwidth   6.5 in
\textheight  8.5 in


\begin{document}

\title{ Spectral Clustering Implementation and Appliacation }
\author{Qi Wang ~~~ Hanqiu Xia}
\maketitle

\abstract{Spectral clustering is widely used in image segmentation. The main idea of spectral clustering is to use the spectrum (eigenvalues) of the similarity matrix of the data to perform dimensionality reduction and then perform clustering in fewer dimensions.  In this project, we investigated the normalized spectral decomposition algorithm from the paper "On Spectral Clustering: Analysis and an algorithm" by A. Ng, M. Jordan, and Y. Weiss [1]. Their algorithm improved upon the existing spectral clustering algorithm by resolving the issues of inconsistent algorithms of using eigenvectors as well as providing evidence of it resulting in a reasonable clustering. We implemented the algorithm using Python with considering both common and edge cases. We also optimized the Python codes by applying vectorization, Cython, and Just-In-Time compiling. Finally, we tested the algorithm on simulated and real datasets and compare the experimental results with those of k-means clustering to see if spectral clustering dramatically improves the results.}

\section{Background}

The research paper we have chosen is "On Spectral Clustering: Analysis and an algorithm" by A. Ng, M. Jordan, and Y. Weiss [1]. Spectral clustering is inspired by the the idea of spectral graph partitioning, in which we use the first two eigenvectors to partition the graph into exactly two parts. Spectral clustering is widely used in image segmentation. According to Ng et al [1].,  the basic idea of spectral clustering is utilising the  $k$ eigenvectors simultaneously to cluster points into  $k$ subsets. The detailed algorithm is as follows:\\[10pt]
\qquad Suppose we have a set of $n$ points $S = \{ s_1, \ldots, s_n\}$ in ${\rm I\!R}^{m}$, and we want to cluster them into $k$ groups.\\

\textbf{Step 1:} Construct the affinity matrix $A \in{\rm I\!R}^{n\times n} $,  each element in A is defined as $A_{ij} = exp(-||s_i-s_j||^2/2\sigma^2)$,  for $i, j = 1,\ldots, n$. We will introduce a method of choosing $\sigma$ in later pages.\\

\textbf{Step 2:} Define $D$ to be the diagonal matrix with $D_{ii} =\sum_{j=1}^{n} A _{ij}$, and form the normalized Laplacian matrix $L = D^{-1/2}AD^{-1/2}$.\\

\textbf{Step 3:} Capture the fist $k$ largest eigenvectors of $L, e_1, e_2,  \ldots, e_k, e_i \in {\rm I\!R}^{n} $ and form the matrix $E = [e_1 ~e_2 ~\cdots ~e_k ] \in  {\rm I\!R}^{n\times k}.$\\

\textbf{Step 4:} Create the new normalized matrix $U \in {\rm I\!R}^{n\times k}$ from $E$, defined as $U_{ij}= E_{ij}/(\sum_{j=1}^{n}E_{ij}^2)^{1/2}$. \\

\textbf{Step 5:} Consider $U$ to be a set of $n$   points that need to be clustered now, apply K-means or any other algorithm that can minimize distortion to cluster $U$. \\

\textbf{Step 6:}  Assign the original point $s_i$ in to cluster $j$ if and only if the $i$th-row of $U$ was distributed to cluster $j$  in previous step.       \\

This algorithm improves upon the spectral clustering algorithm by suggesting the normalized spectral decomposition in forming the affinity matrix, and it resolves the problem of inconsistent algorithms of using eigenvectors in spectral clustering. When applying the algorithm to data that does not have a clear segmentation, like the natural clusters do not correspond to convex region, it performs much better than the other clustering algorithms. In the following sections, we will implement the spectral clustering algorithm and optimize the coding, then we will utilize it on real datasets and compare the results with other clustering mechanisms such as K-means clustering.



\section{Implementation}
\subsection{Ideal Case}
The ideal case denotes the situation when the clusters are clearly partitioned and are far apart from each other. The proposed mechanism works very well in this case and will result in an exact match to the true clustering of the original data.
\subsection{General Case}


\section{Testing}
\subsection{Unit  Test for Common Case}
\subsection{Unit Test for Edge Case}

\section{Optimization}
We used vectorization, cython, and Just-In-Time compiling to optimize our code.
\subsection{Vectorization}
To eliminate the use of for loops, we vectorized to code to improve the performance. If we have a large dataset to perform clustering, the runtime will improve consistently by at least a factor of two.
\subsection{JIT (Just-In-Time compiling) }
We also used JIT to improve the performance, and as a result, JIT improves the runtime the most. It improved by around 20 fold.
\subsection{Cython}
Cython does not perform quite as well compared to the above two methods. It only improves the runtime by around a factor of 1.5.

\subsection{Improvement result}
We can see that JIT produces the best results in optimization. Thus, we will use JIT in the following applications.

\section{Application and Comparison}
\subsection{Application on Simulated Data}
The first set of simulated data we used fall under the ideal case. The data points are well segmented and from the graph we can see they form three clusters perfectly.
\subsection{Application on Real Data}
\subsection{Comparison of Sepctral clustering and K-means}



\section{Conclusion}

%\bibliographystyle{plain}
%\bibliography{bibliography}


\newpage
\begin{thebibliography}{9}



\bibitem{nips} 
A. Ng, M. Jordan, and Y. Weiss.
On Spectral Clustering: Analysis and an algorithm.
\textit{Advances in Neural Information Processing}.
Vol. 14, No. 2. (2001), pp. 849-856 

\end{thebibliography}



\end{document}